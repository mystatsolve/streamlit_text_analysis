# -*- coding: utf-8 -*-
"""
LDA & BERTopic í† í”½ ë¶„ì„ ì „ìš© Streamlit ì•±
"""

import os
import io
import re
import warnings

os.environ.setdefault("PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION", "python")
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import plotly.express as px
import plotly.graph_objects as go

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="í† í”½ ë¶„ì„ ë„êµ¬",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ í•œêµ­ì–´ í°íŠ¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_korean_font():
    candidates = [
        r"C:\Windows\Fonts\malgun.ttf",
        r"C:\Windows\Fonts\malgunbd.ttf",
        r"C:\Windows\Fonts\gulim.ttc",
        r"C:\Windows\Fonts\batang.ttc",
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    found = fm.findSystemFonts()
    for f in found:
        fn = os.path.basename(f).lower()
        if any(k in fn for k in ["malgun", "nanum", "gulim", "batang", "dotum"]):
            return f
    return None

_font_path = get_korean_font()
FONT_NAME = "sans-serif"
if _font_path:
    fm.fontManager.addfont(_font_path)
    _prop = fm.FontProperties(fname=_font_path)
    FONT_NAME = _prop.get_name()
    plt.rcParams["font.family"] = FONT_NAME
    plt.rcParams["font.sans-serif"] = [FONT_NAME] + plt.rcParams.get("font.sans-serif", [])
plt.rcParams["axes.unicode_minus"] = False

# â”€â”€ LDA ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from gensim import corpora
    from gensim.models import LdaModel
    import pyLDAvis
    import pyLDAvis.gensim_models as gensimvis
    HAS_LDA = True
except ImportError:
    HAS_LDA = False

# â”€â”€ Kiwi ìºì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì¤‘...")
def load_kiwi():
    from kiwipiepy import Kiwi
    return Kiwi()

# â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_STOPWORDS = [
    "ìˆë‹¤", "í•˜ë‹¤", "ë˜ë‹¤", "ì´ë‹¤", "ì—†ë‹¤", "ì•Šë‹¤", "ê°™ë‹¤", "ë³´ë‹¤", "ë‚˜ë‹¤",
    "ê²ƒ", "ë“¤", "ê·¸", "ìˆ˜", "ì´", "ë“±", "ë•Œ", "ë…„", "ê°€", "ë¥¼", "ë¡œ",
    "ì—", "ì˜", "ì€", "ëŠ”", "ê³¼", "ì™€", "ë‹¤", "ì„", "í•œ", "ëŒ€", "ìœ¼ë¡œ",
    "í•˜ëŠ”", "ìœ„", "ë°", "ë˜", "ë˜ëŠ”", "ë”", "ì—¬", "ì¤‘", "í†µí•´", "ìœ„í•´",
    "ë”°ë¼", "ë”°ë¥¸", "ëŒ€í•œ", "ê´€í•œ", "í†µí•œ", "ê¸°ë°˜", "ë³¸", "ì´ëŸ¬í•œ", "ë§¤ìš°",
    "ë°", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "ë”°ë¼ì„œ", "í•˜ì§€ë§Œ", "ê·¸ë˜ì„œ", "ë˜í•œ", "ì¦‰",
    "ë¿", "ê°", "í›„", "ë°”", "ë•Œë¬¸", "ì•Š", "ëª»", "ì´ë¥¼", "ê·¸ê²ƒ", "ì—¬ê¸°",
    "ì—°êµ¬", "ë…¼ë¬¸", "ë¶„ì„", "ê²°ê³¼", "ë°©ë²•", "ëª©ì ", "ëŒ€ìƒ", "ì‚¬ìš©", "í™œìš©",
    "ì œì‹œ", "í™•ì¸", "ì§„í–‰", "ìˆ˜í–‰", "ë‚´ìš©", "í•„ìš”", "ê°€ëŠ¥", "ì œê³µ", "ê°œë°œ",
    "êµ¬ì¶•", "ì„¤ê³„", "ì ìš©", "ë„ì…", "ìš´ì˜", "ì‹¤ì‹œ", "ì¡°ì‚¬", "ê²€í† ", "íŒŒì•…",
    "ì‚´í´", "ë³¸ê³ ", "ë³¸ë¬¸", "ë…¼ì˜", "ê³ ì°°", "íƒìƒ‰", "íƒêµ¬", "ê²€ì¦", "í‰ê°€",
    "ë„ì¶œ", "ì‹œì‚¬ì ", "ì œì–¸", "ì£¼ìš”", "ì¸¡ë©´", "ê³¼ì •", "ì˜ì—­", "ë¶€ë¶„", "ì •ë„",
    "ê²½ìš°", "ì´ìƒ", "ì´í•˜", "ì‚¬ì´", "ì •ë³´", "ìë£Œ", "ê¸°ìˆ ", "ì‹œìŠ¤í…œ",
    "study", "research", "paper", "results", "analysis", "method",
    "data", "based", "using", "used", "this", "that", "the", "and", "for",
]

def fig_to_png(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format="PNG", dpi=150, bbox_inches="tight", facecolor="white")
    buf.seek(0)
    return buf.getvalue()

def df_to_csv_bytes(df):
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")

def detect_columns(df):
    """abstract/year ì»¬ëŸ¼ ìë™ ê°ì§€"""
    text_col, year_col = None, None
    for c in df.columns:
        cl = c.lower()
        if text_col is None and any(k in cl for k in ["abstract", "text", "sentence", "ë‚´ìš©", "ë³¸ë¬¸", "ì´ˆë¡"]):
            text_col = c
        if year_col is None and any(k in cl for k in ["year", "ì—°ë„", "ë…„ë„", "ë…„"]):
            year_col = c
    if text_col is None and len(df.columns) > 0:
        text_col = df.columns[0]
    if year_col is None and len(df.columns) > 1:
        year_col = df.columns[1]
    return text_col, year_col

def extract_nouns(text, kiwi, stopwords_set, min_len=2):
    """Kiwië¡œ ëª…ì‚¬ ì¶”ì¶œ (LDAìš© list ë°˜í™˜)"""
    if pd.isna(text) or not str(text).strip():
        return []
    text = re.sub(r"[a-zA-Z]+", " ", str(text))
    text = re.sub(r"[^ê°€-í£\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        return []
    try:
        result = kiwi.analyze(text)
        nouns = []
        for token, pos, _, _ in result[0][0]:
            if pos in ("NNG", "NNP") and len(token) >= min_len and token not in stopwords_set:
                nouns.append(token)
        return nouns
    except Exception:
        return []

def extract_nouns_str(text, kiwi, stopwords_set, min_len=2):
    """Kiwië¡œ ëª…ì‚¬ ì¶”ì¶œ (BERTopicìš© ê³µë°± êµ¬ë¶„ ë¬¸ìì—´ ë°˜í™˜)"""
    tokens = extract_nouns(text, kiwi, stopwords_set, min_len)
    return " ".join(tokens)

# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.title("ğŸ”¬ í† í”½ ë¶„ì„ ë„êµ¬")
    st.markdown("---")

    st.subheader("ğŸ“‚ ë°ì´í„° íŒŒì¼")
    uploaded = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

    # ê¸°ë³¸ ê²½ë¡œ í›„ë³´
    DEFAULT_DATA_PATH = os.path.join(
        os.path.dirname(os.path.abspath(__file__)),
        "..", "7_topic", "abstract_year_data.csv"
    )
    use_default = st.checkbox(
        "ê¸°ë³¸ ë°ì´í„° ì‚¬ìš© (abstract_year_data.csv)",
        value=(uploaded is None),
        disabled=(uploaded is not None),
    )

    st.markdown("---")
    st.subheader("âš™ï¸ ê³µí†µ ì„¤ì •")
    min_word_len = st.slider("ìµœì†Œ ë‹¨ì–´ ê¸¸ì´", 1, 4, 2)

    with st.expander("ë¶ˆìš©ì–´ í¸ì§‘"):
        sw_text = st.text_area(
            "ë¶ˆìš©ì–´ ëª©ë¡ (ì‰¼í‘œ ë˜ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
            value=", ".join(DEFAULT_STOPWORDS),
            height=200,
        )
    stopwords_set = set(
        w.strip() for w in re.split(r"[,\n]", sw_text) if w.strip()
    )

    st.markdown("---")
    st.caption("LDA: gensim + pyLDAvis\nBERTopic: sentence-transformers + HDBSCAN")

# â”€â”€ ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def load_csv(content_bytes: bytes, filename: str):
    for enc in ("utf-8-sig", "utf-8", "cp949", "euc-kr"):
        try:
            return pd.read_csv(io.BytesIO(content_bytes), encoding=enc)
        except Exception:
            continue
    return None

df_raw = None
load_error = None

if uploaded is not None:
    df_raw = load_csv(uploaded.read(), uploaded.name)
    if df_raw is None:
        load_error = "íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì½”ë”©ì„ í™•ì¸í•˜ì„¸ìš”."
elif use_default:
    if os.path.exists(DEFAULT_DATA_PATH):
        try:
            df_raw = pd.read_csv(DEFAULT_DATA_PATH, encoding="utf-8-sig")
        except Exception:
            try:
                df_raw = pd.read_csv(DEFAULT_DATA_PATH, encoding="cp949")
            except Exception as e:
                load_error = f"ê¸°ë³¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}"
    else:
        load_error = f"ê¸°ë³¸ ë°ì´í„° íŒŒì¼ ì—†ìŒ:\n`{DEFAULT_DATA_PATH}`\n\nCSV íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."

# â”€â”€ ì»¬ëŸ¼ ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
text_col = year_col = None
if df_raw is not None:
    detected_text, detected_year = detect_columns(df_raw)
    col1, col2 = st.columns(2)
    with col1:
        text_col = st.selectbox("í…ìŠ¤íŠ¸ ì»¬ëŸ¼", df_raw.columns.tolist(),
                                index=df_raw.columns.tolist().index(detected_text) if detected_text else 0)
    with col2:
        year_col = st.selectbox("ì—°ë„ ì»¬ëŸ¼", df_raw.columns.tolist(),
                                index=df_raw.columns.tolist().index(detected_year) if detected_year else min(1, len(df_raw.columns)-1))

    df_raw[year_col] = pd.to_numeric(df_raw[year_col], errors="coerce")
    df_work = df_raw[[text_col, year_col]].dropna().copy()
    df_work.columns = ["abstract", "year"]
    df_work["year"] = df_work["year"].astype(int)

    st.info(f"ğŸ“Š ì´ **{len(df_work):,}**ê°œ ë¬¸ì„œ Â· ì—°ë„ ë²”ìœ„: **{df_work['year'].min()}~{df_work['year'].max()}**")
elif load_error:
    st.error(load_error)
    st.stop()
else:
    st.warning("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ ë°ì´í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    st.stop()

# â”€â”€ íƒ­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_lda, tab_bert = st.tabs(["ğŸ” LDA í† í”½ ë¶„ì„", "ğŸ¤– BERTopic ë¶„ì„"])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 1 : LDA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_lda:
    if not HAS_LDA:
        st.error("gensim ë˜ëŠ” pyLDAvisê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n```\npip install gensim pyLDAvis\n```")
    else:
        st.subheader("ğŸ” LDA í† í”½ ë¶„ì„ ì„¤ì •")
        c1, c2, c3, c4 = st.columns(4)
        lda_n_topics   = c1.slider("í† í”½ ìˆ˜",        2, 20, 5)
        lda_passes     = c2.slider("Passes",          5, 30, 15)
        lda_no_below   = c3.slider("ìµœì†Œ ë¬¸ì„œ ë¹ˆë„", 1, 20, 3)
        lda_no_above   = c4.slider("ìµœëŒ€ ë¬¸ì„œ ë¹„ìœ¨", 0.3, 0.99, 0.70, step=0.05, format="%.2f")

        run_lda = st.button("â–¶ LDA ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

        if run_lda:
            st.session_state.pop("lda_result", None)

            try:
                kiwi = load_kiwi()
            except Exception as e:
                st.error(f"í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
                st.stop()

            # ëª…ì‚¬ ì¶”ì¶œ
            prog = st.progress(0, "ëª…ì‚¬ ì¶”ì¶œ ì¤‘...")
            tokens_list = []
            n = len(df_work)
            for i, row in df_work.iterrows():
                tokens_list.append(extract_nouns(row["abstract"], kiwi, stopwords_set, min_word_len))
                if i % max(1, n // 50) == 0:
                    prog.progress(min(int((df_work.index.get_loc(i) + 1) / n * 40), 40),
                                  f"ëª…ì‚¬ ì¶”ì¶œ ì¤‘... ({df_work.index.get_loc(i)+1}/{n})")

            df_work2 = df_work.copy()
            df_work2["tokens"] = tokens_list
            df_work2 = df_work2[df_work2["tokens"].apply(len) > 0].reset_index(drop=True)

            prog.progress(40, "ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„± ì¤‘...")
            dictionary = corpora.Dictionary(df_work2["tokens"].tolist())
            dictionary.filter_extremes(no_below=lda_no_below, no_above=lda_no_above)

            if len(dictionary) < 5:
                st.error("ì‚¬ì „ ë‹¨ì–´ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„°(ìµœì†Œ ë¬¸ì„œ ë¹ˆë„, ìµœëŒ€ ë¬¸ì„œ ë¹„ìœ¨)ë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
                st.stop()

            corpus = [dictionary.doc2bow(t) for t in df_work2["tokens"].tolist()]

            prog.progress(45, f"LDA ëª¨ë¸ í•™ìŠµ ì¤‘ (í† í”½ {lda_n_topics}ê°œ, passes {lda_passes})...")
            lda_model = LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=lda_n_topics,
                random_state=42,
                passes=lda_passes,
                alpha="auto",
                eta="auto",
                chunksize=100,
            )

            prog.progress(80, "í† í”½ í• ë‹¹ ë° ì‹œê°í™” ì¤€ë¹„ ì¤‘...")

            # í† í”½ë³„ í‚¤ì›Œë“œ
            topics_rows = []
            for idx in range(lda_n_topics):
                top10 = lda_model.show_topic(idx, topn=10)
                keywords = [w for w, _ in top10]
                weights  = [round(s, 4) for _, s in top10]
                topics_rows.append({
                    "í† í”½": f"í† í”½ {idx+1}",
                    "ì£¼ìš” í‚¤ì›Œë“œ (ìƒìœ„ 10ê°œ)": ", ".join(keywords),
                    "ê°€ì¤‘ì¹˜": ", ".join(str(w) for w in weights),
                })
            topics_df = pd.DataFrame(topics_rows)

            # ë¬¸ì„œë³„ í† í”½ í• ë‹¹
            doc_topics_rows = []
            for i, bow in enumerate(corpus):
                dist = lda_model.get_document_topics(bow, minimum_probability=0.0)
                dom  = max(dist, key=lambda x: x[1])
                doc_topics_rows.append({
                    "doc_id": i,
                    "year": df_work2.iloc[i]["year"],
                    "dominant_topic": dom[0] + 1,
                    "topic_prob": round(dom[1], 4),
                })
            doc_topics_df = pd.DataFrame(doc_topics_rows)

            # ì—°ë„ë³„ í† í”½ ë¹„ìœ¨
            yearly = doc_topics_df.groupby(["year", "dominant_topic"]).size().unstack(fill_value=0)
            yearly_pct = yearly.div(yearly.sum(axis=1), axis=0) * 100
            yearly_pct.columns = [f"í† í”½{c}" for c in yearly_pct.columns]
            yearly_pct = yearly_pct.reset_index()

            # pyLDAvis HTML
            prog.progress(90, "pyLDAvis ì‹œê°í™” ìƒì„± ì¤‘...")
            try:
                vis_data = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)
                lda_html = pyLDAvis.prepared_data_to_html(vis_data)
            except Exception as e:
                lda_html = f"<p style='color:red'>pyLDAvis ìƒì„± ì‹¤íŒ¨: {e}</p>"

            prog.progress(100, "ì™„ë£Œ!")
            prog.empty()

            st.session_state["lda_result"] = {
                "topics_df": topics_df,
                "doc_topics_df": doc_topics_df,
                "yearly_pct": yearly_pct,
                "lda_html": lda_html,
                "n_topics": lda_n_topics,
                "n_docs": len(df_work2),
                "dict_size": len(dictionary),
            }

        # â”€â”€ ê²°ê³¼ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "lda_result" in st.session_state:
            res = st.session_state["lda_result"]
            topics_df     = res["topics_df"]
            doc_topics_df = res["doc_topics_df"]
            yearly_pct    = res["yearly_pct"]
            lda_html      = res["lda_html"]

            st.markdown("---")

            # ìš”ì•½ ì§€í‘œ
            m1, m2, m3 = st.columns(3)
            m1.metric("í† í”½ ìˆ˜",        res["n_topics"])
            m2.metric("ë¶„ì„ ë¬¸ì„œ ìˆ˜",   f"{res['n_docs']:,}")
            m3.metric("ì‚¬ì „ ë‹¨ì–´ ìˆ˜",   f"{res['dict_size']:,}")

            # í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸”
            st.subheader("ğŸ“‹ í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ")
            st.dataframe(topics_df, use_container_width=True, hide_index=True)
            st.download_button("â¬‡ í† í”½ í‚¤ì›Œë“œ CSV ë‹¤ìš´ë¡œë“œ", df_to_csv_bytes(topics_df),
                               "lda_topics.csv", "text/csv")

            st.markdown("---")
            col_a, col_b = st.columns(2)

            # í† í”½ ë¶„í¬ ë°” ì°¨íŠ¸
            with col_a:
                st.subheader("ğŸ“Š í† í”½ë³„ ë¬¸ì„œ ë¶„í¬")
                dist_df = (doc_topics_df["dominant_topic"]
                           .value_counts().sort_index().reset_index())
                dist_df.columns = ["í† í”½", "ë¬¸ì„œ ìˆ˜"]
                dist_df["í† í”½"] = dist_df["í† í”½"].apply(lambda x: f"í† í”½ {x}")
                fig_dist = px.bar(dist_df, x="ë¬¸ì„œ ìˆ˜", y="í† í”½",
                                  orientation="h", color="ë¬¸ì„œ ìˆ˜",
                                  color_continuous_scale="Blues",
                                  title="í† í”½ë³„ ë¬¸ì„œ ìˆ˜")
                fig_dist.update_layout(yaxis={"categoryorder": "total ascending"},
                                       coloraxis_showscale=False, height=350)
                st.plotly_chart(fig_dist, use_container_width=True)

            # ì—°ë„ë³„ í† í”½ íŠ¸ë Œë“œ
            with col_b:
                st.subheader("ğŸ“… ì—°ë„ë³„ í† í”½ ë¹„ìœ¨ íŠ¸ë Œë“œ")
                topic_cols = [c for c in yearly_pct.columns if c != "year"]
                fig_trend = go.Figure()
                for tc in topic_cols:
                    fig_trend.add_trace(go.Bar(
                        x=yearly_pct["year"], y=yearly_pct[tc], name=tc
                    ))
                fig_trend.update_layout(
                    barmode="stack",
                    xaxis_title="ì—°ë„", yaxis_title="ë¹„ìœ¨ (%)",
                    title="ì—°ë„ë³„ í† í”½ ë¹„ìœ¨ (ëˆ„ì  ë§‰ëŒ€)",
                    legend=dict(orientation="h", y=-0.3),
                    height=350,
                )
                st.plotly_chart(fig_trend, use_container_width=True)

            st.markdown("---")

            # ì—°ë„ë³„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            st.download_button("â¬‡ ì—°ë„ë³„ í† í”½ íŠ¸ë Œë“œ CSV", df_to_csv_bytes(yearly_pct),
                               "lda_yearly_trend.csv", "text/csv")
            st.download_button("â¬‡ ë¬¸ì„œë³„ í† í”½ í• ë‹¹ CSV", df_to_csv_bytes(doc_topics_df),
                               "lda_document_topics.csv", "text/csv")

            # pyLDAvis ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
            st.subheader("ğŸŒ pyLDAvis ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”")
            st.caption("ë²„ë¸”ì„ í´ë¦­í•˜ë©´ í•´ë‹¹ í† í”½ì˜ í‚¤ì›Œë“œ ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            components.html(lda_html, height=800, scrolling=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 2 : BERTopic
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_bert:
    st.subheader("ğŸ¤– BERTopic ë¶„ì„ ì„¤ì •")

    c1, c2, c3 = st.columns(3)
    bert_min_cluster = c1.slider("ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°",  10, 300, 100, step=10)
    bert_min_samples = c2.slider("ìµœì†Œ ìƒ˜í”Œ ìˆ˜",         3,  50,  15)
    bert_min_topic   = c3.slider("ìµœì†Œ í† í”½ í¬ê¸°",       10, 300, 100, step=10)

    st.caption("âš ï¸ ì„ë² ë”© ëª¨ë¸(paraphrase-multilingual-MiniLM-L12-v2)ì´ ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤. ì•½ 5~10ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    run_bert = st.button("â–¶ BERTopic ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

    if run_bert:
        st.session_state.pop("bert_result", None)

        with st.spinner("BERTopic íŒ¨í‚¤ì§€ ë¡œë“œ ì¤‘..."):
            try:
                from bertopic import BERTopic
                from sentence_transformers import SentenceTransformer
                from sklearn.feature_extraction.text import CountVectorizer as CV
                from hdbscan import HDBSCAN
            except Exception as e:
                st.error(f"BERTopic ì„í¬íŠ¸ ì‹¤íŒ¨:\n```\n{e}\n```")
                st.stop()

        try:
            kiwi = load_kiwi()
        except Exception as e:
            st.error(f"í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.stop()

        # ëª…ì‚¬ ì¶”ì¶œ
        prog = st.progress(0, "ëª…ì‚¬ ì¶”ì¶œ ì¤‘...")
        noun_texts = []
        n = len(df_work)
        for i, row in df_work.iterrows():
            noun_texts.append(extract_nouns_str(row["abstract"], kiwi, stopwords_set, min_word_len))
            loc = df_work.index.get_loc(i)
            if loc % max(1, n // 50) == 0:
                prog.progress(min(int((loc + 1) / n * 30), 30),
                              f"ëª…ì‚¬ ì¶”ì¶œ ì¤‘... ({loc+1}/{n})")

        df_bert = df_work.copy()
        df_bert["nouns"] = noun_texts
        df_bert = df_bert[df_bert["nouns"].str.strip() != ""].reset_index(drop=True)
        docs  = df_bert["nouns"].tolist()
        years = df_bert["year"].astype(int).tolist()

        prog.progress(30, "ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            emb_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        except Exception as e:
            st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.stop()

        prog.progress(40, "BERTopic ëª¨ë¸ í•™ìŠµ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)...")
        try:
            vec_model = CV(ngram_range=(1, 2), min_df=2, max_df=1.0)
            hdbscan_model = HDBSCAN(
                min_cluster_size=bert_min_cluster,
                min_samples=bert_min_samples,
                metric="euclidean",
                cluster_selection_method="eom",
                prediction_data=True,
            )
            topic_model = BERTopic(
                embedding_model=emb_model,
                vectorizer_model=vec_model,
                hdbscan_model=hdbscan_model,
                language="multilingual",
                calculate_probabilities=True,
                verbose=False,
                min_topic_size=bert_min_topic,
            )
            topics, probs = topic_model.fit_transform(docs)
        except Exception as e:
            st.error(f"BERTopic í•™ìŠµ ì‹¤íŒ¨:\n```\n{e}\n```")
            st.stop()

        prog.progress(75, "ê²°ê³¼ ì •ë¦¬ ì¤‘...")

        topic_info = topic_model.get_topic_info()
        valid_topics = topic_info[topic_info["Topic"] != -1]

        # í† í”½ë³„ í‚¤ì›Œë“œ DF
        kw_rows = []
        for _, row in valid_topics.iterrows():
            words = topic_model.get_topic(row["Topic"])
            if words:
                kw_rows.append({
                    "í† í”½ ID": row["Topic"],
                    "ë¬¸ì„œ ìˆ˜": row["Count"],
                    "ìƒìœ„ í‚¤ì›Œë“œ (10ê°œ)": ", ".join(w for w, _ in words[:10]),
                })
        kw_df = pd.DataFrame(kw_rows)

        # ë¬¸ì„œë³„ í† í”½
        df_bert["topic"] = topics
        doc_prob = []
        for p in probs:
            arr = np.asarray(p)
            doc_prob.append(float(arr.max()) if arr.size > 0 else 0.0)
        df_bert["topic_prob"] = doc_prob
        doc_topics_out = df_bert[["abstract", "year", "topic", "topic_prob"]].copy()

        # ì—°ë„ë³„ í† í”½ íŠ¸ë Œë“œ (topics_over_time)
        prog.progress(85, "ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        try:
            tot_df = topic_model.topics_over_time(docs, years, nr_bins=min(8, len(set(years))))
        except Exception:
            tot_df = None

        # Plotly ì‹œê°í™” HTML
        prog.progress(92, "ì‹œê°í™” ìƒì„± ì¤‘...")
        barchart_html = heatmap_html = hierarchy_html = None
        top_n = min(10, len(valid_topics))
        try:
            fig_bc = topic_model.visualize_barchart(top_n_topics=top_n, n_words=8)
            barchart_html = fig_bc.to_html(full_html=False)
        except Exception:
            pass
        try:
            fig_hm = topic_model.visualize_heatmap(top_n_topics=min(15, len(valid_topics)))
            heatmap_html = fig_hm.to_html(full_html=False)
        except Exception:
            pass
        try:
            fig_hi = topic_model.visualize_hierarchy(top_n_topics=min(20, len(valid_topics)))
            hierarchy_html = fig_hi.to_html(full_html=False)
        except Exception:
            pass

        prog.progress(100, "ì™„ë£Œ!")
        prog.empty()

        st.session_state["bert_result"] = {
            "kw_df": kw_df,
            "topic_info": topic_info,
            "doc_topics_out": doc_topics_out,
            "tot_df": tot_df,
            "barchart_html": barchart_html,
            "heatmap_html": heatmap_html,
            "hierarchy_html": hierarchy_html,
            "n_topics": len(valid_topics),
            "n_noise": int(topic_info[topic_info["Topic"] == -1]["Count"].sum()),
            "n_docs": len(df_bert),
        }

    # â”€â”€ ê²°ê³¼ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "bert_result" in st.session_state:
        res = st.session_state["bert_result"]
        kw_df         = res["kw_df"]
        topic_info    = res["topic_info"]
        doc_topics_out = res["doc_topics_out"]
        tot_df        = res["tot_df"]

        st.markdown("---")

        # ìš”ì•½ ì§€í‘œ
        m1, m2, m3 = st.columns(3)
        m1.metric("ë°œê²¬ëœ í† í”½ ìˆ˜",   res["n_topics"])
        m2.metric("ë¶„ì„ ë¬¸ì„œ ìˆ˜",     f"{res['n_docs']:,}")
        m3.metric("ë…¸ì´ì¦ˆ ë¬¸ì„œ ìˆ˜",   f"{res['n_noise']:,}")

        # í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸”
        st.subheader("ğŸ“‹ í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ")
        st.dataframe(kw_df, use_container_width=True, hide_index=True)
        st.download_button("â¬‡ í† í”½ í‚¤ì›Œë“œ CSV ë‹¤ìš´ë¡œë“œ", df_to_csv_bytes(kw_df),
                           "bertopic_keywords.csv", "text/csv")
        st.download_button("â¬‡ ë¬¸ì„œë³„ í† í”½ í• ë‹¹ CSV", df_to_csv_bytes(doc_topics_out),
                           "bertopic_document_topics.csv", "text/csv")

        st.markdown("---")
        col_a, col_b = st.columns(2)

        # í† í”½ ë¶„í¬ ë°”ì°¨íŠ¸
        with col_a:
            st.subheader("ğŸ“Š í† í”½ë³„ ë¬¸ì„œ ë¶„í¬")
            dist_df = (topic_info[topic_info["Topic"] != -1]
                       .sort_values("Count", ascending=False).head(20))
            fig_dist = px.bar(
                dist_df, x="Count", y=dist_df["Topic"].astype(str),
                orientation="h", color="Count",
                color_continuous_scale="Teal",
                labels={"Count": "ë¬¸ì„œ ìˆ˜", "y": "í† í”½ ID"},
                title="í† í”½ë³„ ë¬¸ì„œ ìˆ˜ (ìƒìœ„ 20ê°œ)"
            )
            fig_dist.update_layout(yaxis={"categoryorder": "total ascending"},
                                   coloraxis_showscale=False, height=400)
            st.plotly_chart(fig_dist, use_container_width=True)

        # ì—°ë„ë³„ íŠ¸ë Œë“œ
        with col_b:
            st.subheader("ğŸ“… ì£¼ìš” í† í”½ ì—°ë„ë³„ íŠ¸ë Œë“œ")
            if tot_df is not None and not tot_df.empty:
                top_ids = (topic_info[topic_info["Topic"] != -1]
                           .sort_values("Count", ascending=False).head(6)["Topic"].tolist())
                fig_line = go.Figure()
                for tid in top_ids:
                    sub = tot_df[tot_df["Topic"] == tid]
                    if sub.empty:
                        continue
                    label = f"í† í”½{tid}: {sub['Words'].iloc[0].split(',')[0]}"
                    fig_line.add_trace(go.Scatter(
                        x=sub["Timestamp"], y=sub["Frequency"],
                        mode="lines+markers", name=label,
                    ))
                fig_line.update_layout(
                    xaxis_title="ì—°ë„", yaxis_title="ë¹ˆë„",
                    title="ì£¼ìš” í† í”½ ì—°ë„ë³„ íŠ¸ë Œë“œ (ìƒìœ„ 6ê°œ)",
                    legend=dict(orientation="h", y=-0.35),
                    height=400,
                )
                st.plotly_chart(fig_line, use_container_width=True)
                st.download_button("â¬‡ ì—°ë„ë³„ íŠ¸ë Œë“œ CSV", df_to_csv_bytes(tot_df),
                                   "bertopic_topics_over_time.csv", "text/csv")
            else:
                st.info("ì—°ë„ë³„ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")

        # í† í”½ ë°”ì°¨íŠ¸ (í‚¤ì›Œë“œ)
        if res.get("barchart_html"):
            st.subheader("ğŸ“Š í† í”½ë³„ í‚¤ì›Œë“œ ë°”ì°¨íŠ¸")
            components.html(
                f"<html><head><meta charset='utf-8'></head><body>{res['barchart_html']}</body></html>",
                height=600, scrolling=True
            )

        # íˆíŠ¸ë§µ
        if res.get("heatmap_html"):
            st.subheader("ğŸŒ¡ï¸ í† í”½ ìœ ì‚¬ë„ íˆíŠ¸ë§µ")
            components.html(
                f"<html><head><meta charset='utf-8'></head><body>{res['heatmap_html']}</body></html>",
                height=600, scrolling=True
            )

        # ê³„ì¸µ êµ¬ì¡°
        if res.get("hierarchy_html"):
            st.subheader("ğŸŒ³ í† í”½ ê³„ì¸µ êµ¬ì¡°")
            components.html(
                f"<html><head><meta charset='utf-8'></head><body>{res['hierarchy_html']}</body></html>",
                height=600, scrolling=True
            )
