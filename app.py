# -*- coding: utf-8 -*-
"""
í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ ì•± @AIê°œë°œì ê¹€ìš°ì§„ì´ ë§Œë“¬, AIê°œë°œ ë° ë°ì´í„° ë¶„ì„ ë¬¸ì˜: mystatsolve@gmail.com
- CSV ì—…ë¡œë“œ (sentence/abstract + year ì»¬ëŸ¼)
- Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ëª…ì‚¬ ì¶”ì¶œ
- ë¶ˆìš©ì–´ ì‚¬ìš©ì ì„¤ì •
- ë¹ˆë„ ë¶„ì„í‘œ + ì›Œë“œ í´ë¼ìš°ë“œ
- ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ
"""

import io
import os
import warnings
warnings.filterwarnings("ignore")

# tensorflow(umap ê²½ìœ )ì˜ protobuf ë²„ì „ ì¶©ëŒ ë°©ì§€
os.environ.setdefault("PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION", "python")

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from collections import Counter
from pathlib import Path
from wordcloud import WordCloud
import streamlit as st
import streamlit.components.v1 as components
from kiwipiepy import Kiwi
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from itertools import combinations
import networkx as nx

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="í…ìŠ¤íŠ¸ ë¹ˆë„ ë¶„ì„",
    page_icon="ğŸ“Š",
    layout="wide",
)

# â”€â”€ CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
  .main-title {font-size:2rem; font-weight:800; margin-bottom:.2rem;}
  .sub-title  {color:#64748b; font-size:.95rem; margin-bottom:1.5rem;}
  .step-label {
    background:#3b82f6; color:#fff;
    border-radius:50%; width:24px; height:24px;
    display:inline-flex; align-items:center; justify-content:center;
    font-size:.78rem; font-weight:700; margin-right:6px;
  }
  .section-title {font-size:1.05rem; font-weight:700; margin-bottom:.6rem;}
  div[data-testid="stSidebar"] .stButton > button {width:100%;}
</style>
""", unsafe_allow_html=True)

# â”€â”€ í•œêµ­ì–´ í°íŠ¸ ì„¤ì • (ì›Œë“œí´ë¼ìš°ë“œ & matplotlib) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def get_korean_font():
    """ì‹œìŠ¤í…œì—ì„œ í•œê¸€ ì§€ì› í°íŠ¸ë¥¼ íƒìƒ‰í•´ ê²½ë¡œ ë°˜í™˜"""
    candidates = [
        "C:/Windows/Fonts/malgun.ttf",          # ë§‘ì€ ê³ ë”• (Windows)
        "C:/Windows/Fonts/NanumGothic.ttf",
        "C:/Windows/Fonts/gulim.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",       # Linux (fonts-nanum)
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",  # Linux ëŒ€ì²´
        "/usr/share/fonts/truetype/nanum/NanumMyeongjo.ttf",     # Linux ëŒ€ì²´2
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",            # macOS
    ]
    for p in candidates:
        if Path(p).exists():
            return p
    # matplotlib í°íŠ¸ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•œê¸€ ì§€ì› í°íŠ¸ íƒìƒ‰
    for f in fm.findSystemFonts():
        try:
            prop = fm.FontProperties(fname=f)
            if any(k in prop.get_name() for k in ["Gothic", "ê³ ë”•", "Nanum", "Malgun", "ë‚˜ëˆ”"]):
                return f
        except Exception:
            pass
    return None

FONT_PATH = get_korean_font()
FONT_NAME = "sans-serif"

if FONT_PATH:
    fm.fontManager.addfont(FONT_PATH)
    _prop = fm.FontProperties(fname=FONT_PATH)
    FONT_NAME = _prop.get_name()
    plt.rcParams["font.family"] = FONT_NAME
    plt.rcParams["font.sans-serif"] = [FONT_NAME] + plt.rcParams.get("font.sans-serif", [])
else:
    plt.rcParams["font.family"] = "sans-serif"

plt.rcParams["axes.unicode_minus"] = False

# â”€â”€ Kiwi ì´ˆê¸°í™” (ìºì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë”© ì¤‘â€¦")
def load_kiwi():
    return Kiwi()

# â”€â”€ ê¸°ë³¸ ë¶ˆìš©ì–´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_STOPWORDS = [
    # ê¸°ì¡´ ì½”ë“œ ë¶ˆìš©ì–´
    "ê²ƒ", "ë“±", "ìˆ˜", "ë…„", "ì›”", "ì¼", "ë°”", "ì ", "ì¤‘", "ë‚´", "í›„", "ì „",
    "ê°„", "ìƒ", "ë•Œ", "ê³³", "ë§", "ë¶„", "ëª…", "ê°œ", "íšŒ", "ê¶Œ", "í¸",
    "ë¶€", "ë©´", "ì¥", "ì ˆ", "í•­", "í˜¸", "ì°¨", "ê±´", "ëŒ€", "ìœ„", "í‘œ",
    "ê·¸ë¦¼", "ì°¸ê³ ", "ì˜ˆ", "ë‹¨", "ìª½", "í˜ì´ì§€", "í•™ìˆ ", "ë…¼ë¬¸",
    # ì¶”ê°€ ì¼ë°˜ ë¶ˆìš©ì–´
    "ì´", "ê°€", "ì€", "ëŠ”", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼",
    "ë„", "ë¡œ", "ìœ¼ë¡œ", "ì—ì„œ", "ì—ê²Œ", "ë³´ë‹¤", "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
    "í•˜ë‹¤", "ë˜ë‹¤", "ì•Šë‹¤", "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜í•œ",
    "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ê·¸ëŸ°ë°", "ë³¸", "ê°", "ì—¬ëŸ¬", "ëª¨ë“ ",
]

# â”€â”€ ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOUN_TAGS = {"NNG", "NNP"}  # ì¼ë°˜ëª…ì‚¬, ê³ ìœ ëª…ì‚¬

def extract_nouns(kiwi: Kiwi, texts: list[str], stopwords: set, min_len: int = 2) -> list[str]:
    nouns = []
    for text in texts:
        try:
            result = kiwi.analyze(text)
            for token, tag, _, _ in result[0][0]:
                if tag in NOUN_TAGS and len(token) >= min_len and token not in stopwords:
                    nouns.append(token)
        except Exception:
            pass
    return nouns

# â”€â”€ TF-IDFìš©: ë¬¸ì„œë³„ ëª…ì‚¬ ë¬¸ìì—´ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def texts_to_noun_docs(kiwi: Kiwi, texts: list[str], stopwords: set, min_len: int = 2) -> list[str]:
    """ê° ë¬¸ì„œë¥¼ 'ëª…ì‚¬ ëª…ì‚¬ ëª…ì‚¬â€¦' í˜•íƒœì˜ ë¬¸ìì—´ë¡œ ë³€í™˜ (TfidfVectorizer ì…ë ¥ìš©)"""
    docs = []
    for text in texts:
        try:
            result = kiwi.analyze(str(text))
            nouns = [tok for tok, tag, _, _ in result[0][0]
                     if tag in NOUN_TAGS and len(tok) >= min_len and tok not in stopwords]
            docs.append(" ".join(nouns))
        except Exception:
            docs.append("")
    return docs

# â”€â”€ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_wordcloud(freq_dict: dict, font_path: str, max_words: int, colormap: str):
    wc = WordCloud(
        font_path=font_path if font_path else None,
        background_color="white",
        width=900,
        height=500,
        max_words=max_words,
        colormap=colormap,
        prefer_horizontal=0.9,
    )
    wc.generate_from_frequencies(freq_dict)
    return wc

# â”€â”€ PNG ì €ì¥ í—¬í¼ (ì „ì—­) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_fig_png(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format="PNG", dpi=150, bbox_inches="tight", facecolor="white")
    buf.seek(0)
    return buf.getvalue()

# â”€â”€ ë©”ì¸ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown('<p class="main-title">ğŸ“Š í…ìŠ¤íŠ¸ ë¹ˆë„ ë¶„ì„ ë„êµ¬</p>', unsafe_allow_html=True)
st.markdown('<p class="sub-title">CSV ì—…ë¡œë“œ â†’ ë¶ˆìš©ì–´ ì„¤ì • â†’ ë¹ˆë„ ë¶„ì„ & ì›Œë“œ í´ë¼ìš°ë“œ</p>', unsafe_allow_html=True)
st.markdown(
    '<p style="font-size:0.85rem; color:#475569; margin-top:-0.8rem; margin-bottom:1rem;">'
    'AIê°œë°œì ê¹€ìš°ì§„ ì œì‘ &nbsp;|&nbsp; AIê°œë°œ ë° ë°ì´í„° ë¶„ì„ ì˜ë¢°: '
    '<a href="mailto:mystatsolve@gmail.com" style="color:#3b82f6;">mystatsolve@gmail.com</a>'
    '</p>',
    unsafe_allow_html=True,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: CSV ì—…ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.markdown('<span class="step-label">1</span><span class="section-title">CSV íŒŒì¼ ì—…ë¡œë“œ</span>', unsafe_allow_html=True)

uploaded = st.file_uploader(
    "sentence(ë˜ëŠ” abstract) Â· year ì»¬ëŸ¼ì´ í¬í•¨ëœ CSV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
    type=["csv"],
    label_visibility="collapsed",
)

if uploaded is None:
    st.info("ğŸ“‚ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    st.stop()

# CSV íŒŒì‹±
@st.cache_data(show_spinner=False)
def load_csv(file_bytes: bytes, file_name: str) -> pd.DataFrame:
    for enc in ["utf-8-sig", "utf-8", "cp949", "euc-kr"]:
        try:
            return pd.read_csv(io.BytesIO(file_bytes), encoding=enc)
        except Exception:
            pass
    return pd.DataFrame()

raw_bytes = uploaded.read()
df_raw = load_csv(raw_bytes, uploaded.name)

if df_raw.empty:
    st.error("CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ì½”ë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

st.success(f"âœ… **{uploaded.name}** ë¡œë“œ ì™„ë£Œ â€” {len(df_raw):,}í–‰ Â· {len(df_raw.columns)}ì—´")

# ì»¬ëŸ¼ ìë™ ê°ì§€
TEXT_HINTS = ["sentence", "abstract", "text", "í…ìŠ¤íŠ¸", "ë¬¸ì¥", "ë‚´ìš©", "ì´ˆë¡"]
YEAR_HINTS = ["year", "ì—°ë„", "ë…„ë„", "ë…„", "Year"]

auto_text = next((c for c in df_raw.columns if c.lower() in [h.lower() for h in TEXT_HINTS]), df_raw.columns[0])
auto_year = next((c for c in df_raw.columns if c.lower() in [h.lower() for h in YEAR_HINTS]), df_raw.columns[-1])

c1, c2 = st.columns(2)
with c1:
    text_col = st.selectbox("í…ìŠ¤íŠ¸ ì»¬ëŸ¼", df_raw.columns.tolist(), index=df_raw.columns.tolist().index(auto_text))
with c2:
    year_col = st.selectbox("ì—°ë„ ì»¬ëŸ¼",  df_raw.columns.tolist(), index=df_raw.columns.tolist().index(auto_year))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: ë¶„ì„ ì„¤ì • (ì‚¬ì´ë“œë°”)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with st.sidebar:
    st.markdown("### âš™ï¸ ë¶„ì„ ì„¤ì •")

    # ì—°ë„ í•„í„°
    st.markdown("**ì—°ë„ í•„í„°**")
    all_years = sorted(df_raw[year_col].dropna().astype(str).unique().tolist())
    sel_years = st.multiselect("ë¶„ì„í•  ì—°ë„ (ë¹„ìš°ë©´ ì „ì²´)", all_years, default=[])

    st.divider()

    # ë¹ˆë„ ì„¤ì •
    st.markdown("**ë¶„ì„ ì˜µì…˜**")
    min_freq = st.slider("ìµœì†Œ ë¹ˆë„ìˆ˜",    1, 20, 2)
    min_len  = st.slider("ìµœì†Œ ë‹¨ì–´ ê¸¸ì´", 1,  5, 2)
    top_n    = st.slider("ìƒìœ„ ë‹¨ì–´ ìˆ˜",  10, 300, 100)

    st.divider()

    # ë¶ˆìš©ì–´ ì„¤ì •
    st.markdown("**ë¶ˆìš©ì–´ ì„¤ì •**")
    sw_text = st.text_area(
        "ë¶ˆìš©ì–´ ëª©ë¡ (í•œ ì¤„ì— í•˜ë‚˜ ë˜ëŠ” ì‰¼í‘œ êµ¬ë¶„)",
        value="\n".join(DEFAULT_STOPWORDS),
        height=220,
    )
    stopwords_set = {
        w.strip()
        for line in sw_text.splitlines()
        for w in line.split(",")
        if w.strip()
    }
    st.caption(f"ë“±ë¡ëœ ë¶ˆìš©ì–´: {len(stopwords_set)}ê°œ")

    st.divider()

    # ì›Œë“œí´ë¼ìš°ë“œ ì„¤ì •
    st.markdown("**ì›Œë“œ í´ë¼ìš°ë“œ ì„¤ì •**")
    wc_max   = st.slider("ì›Œë“œ í´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜", 20, 200, 80)
    wc_cmap  = st.selectbox(
        "ìƒ‰ìƒ í…Œë§ˆ",
        ["Blues", "viridis", "plasma", "Set2", "tab10", "RdYlBu", "coolwarm"],
    )

    st.divider()
    run_btn = st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: ë¶„ì„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if not run_btn:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •ì„ ì™„ë£Œí•˜ê³  **ë¶„ì„ ì‹œì‘** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# ì—°ë„ í•„í„° ì ìš©
df_work = df_raw.copy()
df_work[year_col] = df_work[year_col].astype(str).str.strip()
if sel_years:
    df_work = df_work[df_work[year_col].isin(sel_years)]

texts = df_work[text_col].dropna().astype(str).tolist()

if not texts:
    st.error("ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì—°ë„ í•„í„° ë˜ëŠ” ì»¬ëŸ¼ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# Kiwi ë¡œë“œ & ëª…ì‚¬ ì¶”ì¶œ
kiwi = load_kiwi()

with st.spinner(f"ğŸ” {len(texts):,}ê°œ ë¬¸ì„œì—ì„œ ëª…ì‚¬ ì¶”ì¶œ ì¤‘â€¦ (ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)"):
    all_nouns = extract_nouns(kiwi, texts, stopwords_set, min_len)

if not all_nouns:
    st.warning("ì¶”ì¶œëœ ëª…ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ ëª©ë¡ì´ë‚˜ ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
    st.stop()

# ë¹ˆë„ ê³„ì‚°
counter = Counter(all_nouns)
filtered = [(w, f) for w, f in counter.most_common() if f >= min_freq][:top_n]

if not filtered:
    st.warning(f"ìµœì†Œ ë¹ˆë„ {min_freq} ì´ìƒì¸ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ ë¹ˆë„ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")
    st.stop()

total_freq = sum(f for _, f in filtered)
freq_df = pd.DataFrame(
    [{"ìˆœìœ„": i+1, "ë‹¨ì–´": w, "ë¹ˆë„": f, "ë¹„ìœ¨(%)": round(f / total_freq * 100, 2)}
     for i, (w, f) in enumerate(filtered)]
)

# â”€â”€ í†µê³„ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
m1, m2, m3, m4 = st.columns(4)
m1.metric("ë¶„ì„ ë¬¸ì„œ ìˆ˜",  f"{len(texts):,}")
m2.metric("ì „ì²´ ëª…ì‚¬ ìˆ˜",  f"{len(all_nouns):,}")
m3.metric("ê³ ìœ  ë‹¨ì–´ ìˆ˜",  f"{len(counter):,}")
m4.metric("ë¶„ì„ ì—°ë„ ìˆ˜",  len(sel_years) if sel_years else len(all_years))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 4: ê²°ê³¼ íƒ­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tab_freq, tab_wc, tab_yearly, tab_tfidf, tab_cooc = st.tabs([
    "ğŸ“‹ ë¹ˆë„ ë¶„ì„í‘œ", "â˜ï¸ ì›Œë“œ í´ë¼ìš°ë“œ", "ğŸ“… ì—°ë„ë³„ ë¶„ì„", "ğŸ“ˆ TF-IDF ë¶„ì„", "ğŸ•¸ï¸ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬",
])

# â”€â”€ ë¹ˆë„ ë¶„ì„í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_freq:
    col_left, col_right = st.columns([3, 1])
    with col_left:
        st.markdown(f"**ìƒìœ„ {len(freq_df)}ê°œ ë‹¨ì–´**")
    with col_right:
        csv_bytes = freq_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            label="â¬‡ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_bytes,
            file_name="ë¹ˆë„ë¶„ì„_ê²°ê³¼.csv",
            mime="text/csv",
        )

    # ë§‰ëŒ€ ê·¸ë˜í”„ (ìƒìœ„ 30ê°œ)
    top30 = freq_df.head(30)
    fig, ax = plt.subplots(figsize=(10, max(4, len(top30) * 0.32)))
    bars = ax.barh(top30["ë‹¨ì–´"][::-1], top30["ë¹ˆë„"][::-1], color="#3b82f6", alpha=0.85)
    ax.bar_label(bars, padding=3, fontsize=8)
    ax.set_xlabel("ë¹ˆë„")
    ax.set_title(f"ìƒìœ„ {len(top30)}ê°œ ë¹ˆì¶œ ë‹¨ì–´", fontsize=13, fontweight="bold", pad=10)
    ax.spines[["top", "right"]].set_visible(False)
    plt.tight_layout()
    st.pyplot(fig)
    plt.close(fig)

    # ë°ì´í„° í…Œì´ë¸”
    st.dataframe(
        freq_df,
        column_config={
            "ìˆœìœ„": st.column_config.NumberColumn(width="small"),
            "ë¹ˆë„": st.column_config.NumberColumn(format="%d"),
            "ë¹„ìœ¨(%)": st.column_config.NumberColumn(format="%.2f"),
        },
        use_container_width=True,
        hide_index=True,
        height=400,
    )

# â”€â”€ ì›Œë“œ í´ë¼ìš°ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab_wc:
    if FONT_PATH is None:
        st.warning("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›Œë“œ í´ë¼ìš°ë“œì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    with st.spinner("ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘â€¦"):
        freq_dict = {row["ë‹¨ì–´"]: row["ë¹ˆë„"] for _, row in freq_df.head(wc_max).iterrows()}
        try:
            wc = make_wordcloud(freq_dict, FONT_PATH, wc_max, wc_cmap)
            fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
            ax_wc.imshow(wc, interpolation="bilinear")
            ax_wc.axis("off")
            plt.tight_layout(pad=0)
            st.pyplot(fig_wc)
            plt.close(fig_wc)

            # PNG ë‹¤ìš´ë¡œë“œ
            wc_buf = io.BytesIO()
            wc.to_image().save(wc_buf, format="PNG")
            st.download_button(
                label="ğŸ–¼ ì›Œë“œ í´ë¼ìš°ë“œ PNG ì €ì¥",
                data=wc_buf.getvalue(),
                file_name="ì›Œë“œí´ë¼ìš°ë“œ.png",
                mime="image/png",
            )
        except Exception as e:
            st.error(f"ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 5: ì—°ë„ë³„ ë¶„ì„ íƒ­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_yearly:

    # ë¶„ì„ ëŒ€ìƒ ì—°ë„ ëª©ë¡ (í•„í„° ì ìš©ëœ ë°ì´í„° ê¸°ì¤€)
    yearly_targets = sorted(df_work[year_col].dropna().astype(str).str.strip().unique().tolist())

    if len(yearly_targets) == 0:
        st.warning("ë¶„ì„ ê°€ëŠ¥í•œ ì—°ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # â”€â”€ ì—°ë„ë³„ ëª…ì‚¬ ì¶”ì¶œ & ë¹ˆë„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.spinner(f"ğŸ“… {len(yearly_targets)}ê°œ ì—°ë„ë³„ ë¶„ì„ ì¤‘â€¦"):
        yearly_results = {}   # {year: Counter}
        all_yearly_rows = []  # ìƒìœ„30 CSVìš©

        for yr in yearly_targets:
            yr_texts = (
                df_work[df_work[year_col].astype(str).str.strip() == yr][text_col]
                .dropna().astype(str).tolist()
            )
            yr_nouns = extract_nouns(kiwi, yr_texts, stopwords_set, min_len)
            yr_counter = Counter(yr_nouns)
            yearly_results[yr] = yr_counter

            total_yr = len(yr_nouns)
            for rank, (noun, freq) in enumerate(yr_counter.most_common(30), 1):
                ratio = round(freq / total_yr * 100, 2) if total_yr > 0 else 0
                all_yearly_rows.append({
                    "ì—°ë„": yr, "ìˆœìœ„": rank,
                    "ë‹¨ì–´": noun, "ë¹ˆë„": freq, "ë¹„ìœ¨(%)": ratio,
                })

    # â”€â”€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dl1, dl2 = st.columns(2)

    # ì—°ë„ë³„ ìƒìœ„ 30 CSV
    yearly_df = pd.DataFrame(all_yearly_rows)
    with dl1:
        st.download_button(
            label="â¬‡ ì—°ë„ë³„ ìƒìœ„ 30 CSV",
            data=yearly_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name="ë¹ˆë„ë¶„ì„_ì—°ë„ë³„_ìƒìœ„30.csv",
            mime="text/csv",
            use_container_width=True,
        )

    # í”¼ë²— í…Œì´ë¸” CSV (ë‹¨ì–´ Ã— ì—°ë„)
    all_pivot_words = set()
    for yr, cnt in yearly_results.items():
        for w, _ in cnt.most_common(50):
            all_pivot_words.add(w)

    pivot_rows = []
    for word in all_pivot_words:
        row = {"ë‹¨ì–´": word}
        for yr in yearly_targets:
            row[str(yr)] = yearly_results[yr].get(word, 0)
        pivot_rows.append(row)

    pivot_df = pd.DataFrame(pivot_rows)
    pivot_df["ì´í•©"] = pivot_df[[str(y) for y in yearly_targets]].sum(axis=1)
    pivot_df = pivot_df.sort_values("ì´í•©", ascending=False).drop("ì´í•©", axis=1)

    with dl2:
        st.download_button(
            label="â¬‡ ì—°ë„ë³„ í”¼ë²— í…Œì´ë¸” CSV",
            data=pivot_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name="ë¹ˆë„ë¶„ì„_ì—°ë„ë³„_í”¼ë²—.csv",
            mime="text/csv",
            use_container_width=True,
        )

    st.divider()

    # â”€â”€ ì—°ë„ë³„ ìƒìœ„ 15 ë‹¨ì–´ ë§‰ëŒ€ ê·¸ë˜í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### ğŸ“Š ì—°ë„ë³„ ìƒìœ„ 15ê°œ ë¹ˆì¶œ ë‹¨ì–´")

    n_cols = 2
    yr_chunks = [yearly_targets[i:i+n_cols] for i in range(0, len(yearly_targets), n_cols)]

    for chunk in yr_chunks:
        cols = st.columns(n_cols)
        for ci, yr in enumerate(chunk):
            yr_counter = yearly_results[yr]
            top15 = yr_counter.most_common(15)
            if not top15:
                with cols[ci]:
                    st.caption(f"{yr}ë…„: ë°ì´í„° ì—†ìŒ")
                continue

            words  = [w for w, _ in top15][::-1]
            freqs  = [f for _, f in top15][::-1]
            max_f  = max(freqs)

            fig_yr, ax_yr = plt.subplots(figsize=(5, max(3, len(words) * 0.35)))
            bars_yr = ax_yr.barh(words, freqs, color="#3b82f6", alpha=0.82)
            ax_yr.bar_label(bars_yr, padding=3, fontsize=7)
            ax_yr.set_title(f"{yr}ë…„ ìƒìœ„ {len(words)}ê°œ", fontsize=11, fontweight="bold")
            ax_yr.set_xlim(0, max_f * 1.18)
            ax_yr.spines[["top", "right"]].set_visible(False)
            ax_yr.tick_params(labelsize=8)
            plt.tight_layout()

            with cols[ci]:
                st.pyplot(fig_yr)
            plt.close(fig_yr)

    st.divider()

    # â”€â”€ ì—°ë„ë³„ ì›Œë“œ í´ë¼ìš°ë“œ ê·¸ë¦¬ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### â˜ï¸ ì—°ë„ë³„ ì›Œë“œ í´ë¼ìš°ë“œ")

    if FONT_PATH is None:
        st.warning("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•´ ì›Œë“œ í´ë¼ìš°ë“œ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    wc_cols = st.columns(n_cols)
    for ci, yr in enumerate(yearly_targets):
        yr_freq = dict(yearly_results[yr].most_common(wc_max))
        if not yr_freq:
            continue
        try:
            wc_yr = WordCloud(
                font_path=FONT_PATH if FONT_PATH else None,
                background_color="white",
                width=700, height=420,
                max_words=wc_max,
                colormap=wc_cmap,
                prefer_horizontal=0.9,
            ).generate_from_frequencies(yr_freq)

            fig_wc_yr, ax_wc_yr = plt.subplots(figsize=(6, 3.5))
            ax_wc_yr.imshow(wc_yr, interpolation="bilinear")
            ax_wc_yr.axis("off")
            ax_wc_yr.set_title(f"{yr}ë…„ ì£¼ìš” í‚¤ì›Œë“œ", fontsize=11, fontweight="bold", pad=6)
            plt.tight_layout(pad=0.5)

            with wc_cols[ci % n_cols]:
                st.pyplot(fig_wc_yr)

                # ì—°ë„ë³„ PNG ë‹¤ìš´ë¡œë“œ
                buf_yr = io.BytesIO()
                wc_yr.to_image().save(buf_yr, format="PNG")
                st.download_button(
                    label=f"ğŸ–¼ {yr}ë…„ PNG ì €ì¥",
                    data=buf_yr.getvalue(),
                    file_name=f"ì›Œë“œí´ë¼ìš°ë“œ_{yr}.png",
                    mime="image/png",
                    use_container_width=True,
                )
            plt.close(fig_wc_yr)
        except Exception as e:
            with wc_cols[ci % n_cols]:
                st.error(f"{yr}ë…„ ì›Œë“œ í´ë¼ìš°ë“œ ì˜¤ë¥˜: {e}")

    st.divider()

    # â”€â”€ ì „ì²´ í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### â˜ï¸ ì „ì²´ ê¸°ê°„ í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ")

    all_combined = Counter()
    for cnt in yearly_results.values():
        all_combined.update(cnt)

    if all_combined:
        try:
            wc_all = WordCloud(
                font_path=FONT_PATH if FONT_PATH else None,
                background_color="white",
                width=1100, height=600,
                max_words=100,
                colormap=wc_cmap,
                prefer_horizontal=0.9,
            ).generate_from_frequencies(dict(all_combined.most_common(100)))

            fig_all, ax_all = plt.subplots(figsize=(13, 6))
            ax_all.imshow(wc_all, interpolation="bilinear")
            ax_all.axis("off")
            yr_range = f"{min(yearly_targets)}~{max(yearly_targets)}"
            ax_all.set_title(f"{yr_range}ë…„ ì „ì²´ ì£¼ìš” í‚¤ì›Œë“œ", fontsize=14, fontweight="bold", pad=8)
            plt.tight_layout(pad=0.5)
            st.pyplot(fig_all)
            plt.close(fig_all)

            buf_all = io.BytesIO()
            wc_all.to_image().save(buf_all, format="PNG")
            st.download_button(
                label="ğŸ–¼ ì „ì²´ í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ PNG ì €ì¥",
                data=buf_all.getvalue(),
                file_name="ì›Œë“œí´ë¼ìš°ë“œ_ì „ì²´.png",
                mime="image/png",
            )
        except Exception as e:
            st.error(f"ì „ì²´ ì›Œë“œ í´ë¼ìš°ë“œ ì˜¤ë¥˜: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 6: TF-IDF ë¶„ì„ íƒ­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_tfidf:

    # â”€â”€ ë¬¸ì„œë³„ ëª…ì‚¬ ë¬¸ìì—´ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.spinner(f"ğŸ“ˆ TF-IDF ë¶„ì„ì„ ìœ„í•œ ëª…ì‚¬ ì¶”ì¶œ ì¤‘â€¦"):
        noun_docs = texts_to_noun_docs(kiwi, texts, stopwords_set, min_len)

    valid_idx  = [i for i, d in enumerate(noun_docs) if d.strip()]
    valid_docs = [noun_docs[i] for i in valid_idx]

    if len(valid_docs) < 2:
        st.warning("TF-IDF ë¶„ì„ì„ ìœ„í•œ ìœ íš¨ ë¬¸ì„œê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì„¹ì…˜ A: ì „ì²´ TF-IDF
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    st.markdown("#### ğŸ“ˆ ì „ì²´ TF-IDF ë¶„ì„")

    with st.spinner("ì „ì²´ TF-IDF ê³„ì‚° ì¤‘â€¦"):
        vec_all = TfidfVectorizer(max_features=500)
        mat_all = vec_all.fit_transform(valid_docs)
        feat_all   = vec_all.get_feature_names_out()
        means_all  = mat_all.mean(axis=0).A1

    tfidf_all_df = (
        pd.DataFrame({"ë‹¨ì–´": feat_all, "TF-IDF í‰ê· ": means_all.round(6)})
        .sort_values("TF-IDF í‰ê· ", ascending=False)
        .reset_index(drop=True)
    )
    tfidf_all_df.insert(0, "ìˆœìœ„", range(1, len(tfidf_all_df) + 1))
    tfidf_top = tfidf_all_df.head(top_n)

    # ë‹¤ìš´ë¡œë“œ
    dl_tfidf_all, _ = st.columns([1, 3])
    with dl_tfidf_all:
        st.download_button(
            label="â¬‡ ì „ì²´ TF-IDF CSV",
            data=tfidf_all_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name="tfidf_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv",
            use_container_width=True,
        )

    # ë§‰ëŒ€ ê·¸ë˜í”„ (ìƒìœ„ 30)
    t30 = tfidf_top.head(30)
    fig_t, ax_t = plt.subplots(figsize=(10, max(4, len(t30) * 0.32)))
    bars_t = ax_t.barh(t30["ë‹¨ì–´"][::-1], t30["TF-IDF í‰ê· "][::-1], color="#8b5cf6", alpha=0.85)
    ax_t.bar_label(bars_t, fmt="%.4f", padding=3, fontsize=7)
    ax_t.set_xlabel("TF-IDF í‰ê· ")
    ax_t.set_title(f"ì „ì²´ TF-IDF ìƒìœ„ {len(t30)}ê°œ ë‹¨ì–´", fontsize=13, fontweight="bold", pad=10)
    ax_t.spines[["top", "right"]].set_visible(False)
    plt.tight_layout()
    st.pyplot(fig_t)
    plt.close(fig_t)

    # ë°ì´í„° í…Œì´ë¸”
    st.dataframe(
        tfidf_top,
        column_config={
            "ìˆœìœ„":       st.column_config.NumberColumn(width="small"),
            "TF-IDF í‰ê· ": st.column_config.NumberColumn(format="%.6f"),
        },
        use_container_width=True,
        hide_index=True,
        height=380,
    )

    # ì „ì²´ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ
    st.markdown("##### â˜ï¸ ì „ì²´ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ")
    try:
        wc_t_all = WordCloud(
            font_path=FONT_PATH if FONT_PATH else None,
            background_color="white",
            width=1100, height=550,
            max_words=100,
            colormap=wc_cmap,
            prefer_horizontal=0.9,
        ).generate_from_frequencies(
            dict(zip(tfidf_all_df["ë‹¨ì–´"].head(100), tfidf_all_df["TF-IDF í‰ê· "].head(100)))
        )
        fig_twc, ax_twc = plt.subplots(figsize=(13, 6))
        ax_twc.imshow(wc_t_all, interpolation="bilinear")
        ax_twc.axis("off")
        plt.tight_layout(pad=0)
        st.pyplot(fig_twc)
        plt.close(fig_twc)

        buf_tall = io.BytesIO()
        wc_t_all.to_image().save(buf_tall, format="PNG")
        st.download_button(
            label="ğŸ–¼ ì „ì²´ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ PNG",
            data=buf_tall.getvalue(),
            file_name="tfidf_ì›Œë“œí´ë¼ìš°ë“œ_ì „ì²´.png",
            mime="image/png",
        )
    except Exception as e:
        st.error(f"ì „ì²´ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ ì˜¤ë¥˜: {e}")

    st.divider()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì„¹ì…˜ B: ì—°ë„ë³„ TF-IDF
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    st.markdown("#### ğŸ“… ì—°ë„ë³„ TF-IDF ë¶„ì„")

    tfidf_yearly_targets = sorted(
        df_work[year_col].dropna().astype(str).str.strip().unique().tolist()
    )

    with st.spinner(f"ì—°ë„ë³„ TF-IDF ê³„ì‚° ì¤‘â€¦ ({len(tfidf_yearly_targets)}ê°œ ì—°ë„)"):
        yearly_tfidf   = {}   # {year: [(word, score), ...]}
        tfidf_yr_rows  = []   # ì—°ë„ë³„ ìƒìœ„30 CSVìš©
        tfidf_yr_pivot = []   # í”¼ë²— CSVìš©

        for yr in tfidf_yearly_targets:
            yr_mask  = df_work[year_col].astype(str).str.strip() == yr
            yr_texts = df_work[yr_mask][text_col].dropna().astype(str).tolist()
            yr_docs  = texts_to_noun_docs(kiwi, yr_texts, stopwords_set, min_len)
            yr_valid = [d for d in yr_docs if d.strip()]

            if len(yr_valid) < 2:
                continue

            vec_yr  = TfidfVectorizer(max_features=100)
            mat_yr  = vec_yr.fit_transform(yr_valid)
            feat_yr = vec_yr.get_feature_names_out()
            mean_yr = mat_yr.mean(axis=0).A1

            yr_sorted = sorted(zip(feat_yr, mean_yr), key=lambda x: x[1], reverse=True)
            yearly_tfidf[yr] = yr_sorted

            for rank, (word, score) in enumerate(yr_sorted[:30], 1):
                tfidf_yr_rows.append({"ì—°ë„": yr, "ìˆœìœ„": rank, "ë‹¨ì–´": word, "TF-IDF": round(score, 6)})
            for word, score in yr_sorted:
                tfidf_yr_pivot.append({"ì—°ë„": yr, "ë‹¨ì–´": word, "TF-IDF": round(score, 6)})

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    if tfidf_yr_rows:
        tfidf_yr_df = pd.DataFrame(tfidf_yr_rows)
        tfidf_pv_df = (
            pd.DataFrame(tfidf_yr_pivot)
            .pivot_table(index="ë‹¨ì–´", columns="ì—°ë„", values="TF-IDF", fill_value=0)
            .assign(í‰ê· =lambda d: d.mean(axis=1))
            .sort_values("í‰ê· ", ascending=False)
            .round(6)
        )

        dl_yr1, dl_yr2 = st.columns(2)
        with dl_yr1:
            st.download_button(
                label="â¬‡ ì—°ë„ë³„ TF-IDF ìƒìœ„ 30 CSV",
                data=tfidf_yr_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                file_name="tfidf_ì—°ë„ë³„_ìƒìœ„30.csv",
                mime="text/csv",
                use_container_width=True,
            )
        with dl_yr2:
            st.download_button(
                label="â¬‡ ì—°ë„ë³„ TF-IDF í”¼ë²— CSV",
                data=tfidf_pv_df.to_csv(encoding="utf-8-sig").encode("utf-8-sig"),
                file_name="tfidf_ì—°ë„ë³„_í”¼ë²—.csv",
                mime="text/csv",
                use_container_width=True,
            )

        st.divider()

        # ì—°ë„ë³„ TF-IDF ë§‰ëŒ€ ê·¸ë˜í”„
        st.markdown("##### ğŸ“Š ì—°ë„ë³„ TF-IDF ìƒìœ„ 15ê°œ ë‹¨ì–´")
        n_cols = 2
        yr_chunks = [tfidf_yearly_targets[i:i+n_cols] for i in range(0, len(tfidf_yearly_targets), n_cols)]

        for chunk in yr_chunks:
            cols = st.columns(n_cols)
            for ci, yr in enumerate(chunk):
                if yr not in yearly_tfidf:
                    with cols[ci]:
                        st.caption(f"{yr}ë…„: ë°ì´í„° ë¶€ì¡±")
                    continue
                top15 = yearly_tfidf[yr][:15]
                words = [w for w, _ in top15][::-1]
                scores = [s for _, s in top15][::-1]

                fig_ty, ax_ty = plt.subplots(figsize=(5, max(3, len(words) * 0.35)))
                bars_ty = ax_ty.barh(words, scores, color="#8b5cf6", alpha=0.82)
                ax_ty.bar_label(bars_ty, fmt="%.4f", padding=3, fontsize=7)
                ax_ty.set_title(f"{yr}ë…„ TF-IDF ìƒìœ„ {len(words)}ê°œ", fontsize=11, fontweight="bold")
                ax_ty.set_xlim(0, max(scores) * 1.25)
                ax_ty.spines[["top", "right"]].set_visible(False)
                ax_ty.tick_params(labelsize=8)
                plt.tight_layout()
                with cols[ci]:
                    st.pyplot(fig_ty)
                plt.close(fig_ty)

        st.divider()

        # ì—°ë„ë³„ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ ê·¸ë¦¬ë“œ
        st.markdown("##### â˜ï¸ ì—°ë„ë³„ TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ")
        wc_cols = st.columns(n_cols)

        for ci, yr in enumerate(tfidf_yearly_targets):
            if yr not in yearly_tfidf:
                continue
            yr_freq_dict = {w: s for w, s in yearly_tfidf[yr][:wc_max]}
            if not yr_freq_dict:
                continue
            try:
                wc_ty = WordCloud(
                    font_path=FONT_PATH if FONT_PATH else None,
                    background_color="white",
                    width=700, height=420,
                    max_words=wc_max,
                    colormap=wc_cmap,
                    prefer_horizontal=0.9,
                ).generate_from_frequencies(yr_freq_dict)

                fig_wty, ax_wty = plt.subplots(figsize=(6, 3.5))
                ax_wty.imshow(wc_ty, interpolation="bilinear")
                ax_wty.axis("off")
                ax_wty.set_title(f"{yr}ë…„ TF-IDF í‚¤ì›Œë“œ", fontsize=11, fontweight="bold", pad=6)
                plt.tight_layout(pad=0.5)

                with wc_cols[ci % n_cols]:
                    st.pyplot(fig_wty)
                    buf_ty = io.BytesIO()
                    wc_ty.to_image().save(buf_ty, format="PNG")
                    st.download_button(
                        label=f"ğŸ–¼ {yr}ë…„ TF-IDF PNG",
                        data=buf_ty.getvalue(),
                        file_name=f"tfidf_ì›Œë“œí´ë¼ìš°ë“œ_{yr}.png",
                        mime="image/png",
                        use_container_width=True,
                    )
                plt.close(fig_wty)
            except Exception as e:
                with wc_cols[ci % n_cols]:
                    st.error(f"{yr}ë…„ ì˜¤ë¥˜: {e}")

        st.divider()

        # ì—°ë„ë³„ TF-IDF í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ
        st.markdown("##### â˜ï¸ ì „ì²´ ê¸°ê°„ TF-IDF í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ")
        combined_tfidf: dict[str, list] = {}
        for yr, pairs in yearly_tfidf.items():
            for word, score in pairs:
                combined_tfidf.setdefault(word, []).append(score)
        combined_mean = {w: sum(v)/len(v) for w, v in combined_tfidf.items()}
        top_combined  = dict(sorted(combined_mean.items(), key=lambda x: x[1], reverse=True)[:100])

        try:
            wc_tcomb = WordCloud(
                font_path=FONT_PATH if FONT_PATH else None,
                background_color="white",
                width=1100, height=600,
                max_words=100,
                colormap=wc_cmap,
                prefer_horizontal=0.9,
            ).generate_from_frequencies(top_combined)

            fig_tcomb, ax_tcomb = plt.subplots(figsize=(13, 6))
            ax_tcomb.imshow(wc_tcomb, interpolation="bilinear")
            ax_tcomb.axis("off")
            yr_range = f"{min(tfidf_yearly_targets)}~{max(tfidf_yearly_targets)}"
            ax_tcomb.set_title(f"{yr_range}ë…„ TF-IDF ì „ì²´ í‚¤ì›Œë“œ", fontsize=14, fontweight="bold", pad=8)
            plt.tight_layout(pad=0.5)
            st.pyplot(fig_tcomb)
            plt.close(fig_tcomb)

            buf_tcomb = io.BytesIO()
            wc_tcomb.to_image().save(buf_tcomb, format="PNG")
            st.download_button(
                label="ğŸ–¼ TF-IDF í†µí•© ì›Œë“œ í´ë¼ìš°ë“œ PNG",
                data=buf_tcomb.getvalue(),
                file_name="tfidf_ì›Œë“œí´ë¼ìš°ë“œ_ì „ì²´.png",
                mime="image/png",
            )
        except Exception as e:
            st.error(f"í†µí•© TF-IDF ì›Œë“œ í´ë¼ìš°ë“œ ì˜¤ë¥˜: {e}")
    else:
        st.warning("ì—°ë„ë³„ TF-IDF ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 7: ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ íƒ­
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab_cooc:

    # â”€â”€ ì‚¬ì´ë“œë°” ì¶”ê°€ ì„¤ì • ì½ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.divider()
        st.markdown("**ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ì„¤ì •**")
        cooc_top_words = st.slider("ë¶„ì„ ëŒ€ìƒ ìƒìœ„ ë‹¨ì–´ ìˆ˜", 30, 200, 100, key="cooc_top")
        cooc_top_edges = st.slider("ë„¤íŠ¸ì›Œí¬ ì—£ì§€ ìˆ˜ (ì „ì²´)",  50, 300, 150, key="cooc_edges")
        cooc_core_edges= st.slider("í•µì‹¬ ë„¤íŠ¸ì›Œí¬ ì—£ì§€ ìˆ˜",   20, 100,  50, key="cooc_core")

    # â”€â”€ ë¬¸ì„œë³„ ê³ ìœ  ëª…ì‚¬ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.spinner("ğŸ•¸ï¸ ë™ì‹œì¶œí˜„ ë¶„ì„ì„ ìœ„í•œ ëª…ì‚¬ ì¶”ì¶œ ì¤‘â€¦"):
        doc_noun_sets = []
        for text in texts:
            try:
                result = kiwi.analyze(str(text))
                nouns = {tok for tok, tag, _, _ in result[0][0]
                         if tag in NOUN_TAGS and len(tok) >= min_len and tok not in stopwords_set}
                doc_noun_sets.append(nouns)
            except Exception:
                doc_noun_sets.append(set())

    # ì „ì²´ ë¹ˆë„ ê¸°ì¤€ ìƒìœ„ ë‹¨ì–´ ì§‘í•©
    all_flat = [w for ns in doc_noun_sets for w in ns]
    word_freq_all = Counter(all_flat)
    top_word_set  = {w for w, _ in word_freq_all.most_common(cooc_top_words)}

    # â”€â”€ ë™ì‹œì¶œí˜„ ì¹´ìš´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.spinner("ë™ì‹œì¶œí˜„ ìŒ ê³„ì‚° ì¤‘â€¦"):
        cooc_counter = Counter()
        for nouns in doc_noun_sets:
            filtered = sorted(nouns & top_word_set)
            for pair in combinations(filtered, 2):
                cooc_counter[pair] += 1

    if not cooc_counter:
        st.warning("ë™ì‹œì¶œí˜„ ìŒì´ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ ëŒ€ìƒ ë‹¨ì–´ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ë¶ˆìš©ì–´ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")

    # â”€â”€ ì—£ì§€ ë°ì´í„°í”„ë ˆì„ & CSV ë‹¤ìš´ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    edge_rows = [{"ë‹¨ì–´1": p[0], "ë‹¨ì–´2": p[1], "ë™ì‹œì¶œí˜„ ë¹ˆë„": c}
                 for p, c in cooc_counter.most_common()]
    edge_df_cooc = pd.DataFrame(edge_rows)

    st.markdown(f"**ì´ ë™ì‹œì¶œí˜„ ìŒ: {len(edge_df_cooc):,}ê°œ**")

    dl_cooc1, dl_cooc2 = st.columns([1, 3])
    with dl_cooc1:
        st.download_button(
            label="â¬‡ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ CSV",
            data=edge_df_cooc.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            file_name="cooccurrence_edge_list.csv",
            mime="text/csv",
            use_container_width=True,
        )

    # ìƒìœ„ 30 í…Œì´ë¸”
    st.markdown("##### ìƒìœ„ 30ê°œ ë™ì‹œì¶œí˜„ ìŒ")
    top30_cooc = edge_df_cooc.head(30).copy()
    top30_cooc.insert(0, "ìˆœìœ„", range(1, len(top30_cooc) + 1))
    st.dataframe(top30_cooc, use_container_width=True, hide_index=True, height=320)

    st.divider()

    # â”€â”€ ê·¸ë˜í”„ ê³µí†µ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.spinner("ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì¤‘â€¦"):

        # ì „ì²´ ê·¸ë˜í”„ (spring layoutìš©)
        G = nx.Graph()
        for (w1, w2), wt in cooc_counter.most_common(cooc_top_edges):
            G.add_edge(w1, w2, weight=wt)

        pos        = nx.spring_layout(G, k=2.5, iterations=80, seed=42)
        e_widths   = [G[u][v]["weight"] * 0.03 for u, v in G.edges()]
        node_deg   = [G.degree(n) for n in G.nodes()]

        # ì»¤ë®¤ë‹ˆí‹° íƒì§€
        try:
            from networkx.algorithms.community import louvain_communities
            communities = louvain_communities(G, seed=42)
        except Exception:
            from networkx.algorithms.community import greedy_modularity_communities
            communities = list(greedy_modularity_communities(G))

        node_comm = {}
        for ci, comm in enumerate(communities):
            for nd in comm:
                node_comm[nd] = ci
        comm_colors = [node_comm.get(n, 0) for n in G.nodes()]

        degree_cent = nx.degree_centrality(G)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì„ íƒ íƒ­
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    net_tab1, net_tab2, net_tab3, net_tab4, net_tab5 = st.tabs([
        "ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬", "ì—°ê²° ì¤‘ì‹¬ì„±", "ì»¤ë®¤ë‹ˆí‹°", "í•µì‹¬ ë„¤íŠ¸ì›Œí¬", "ì›í˜• ë ˆì´ì•„ì›ƒ"
    ])

    # 1. ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬
    with net_tab1:
        fig1, ax1 = plt.subplots(figsize=(14, 10))
        nx.draw_networkx_edges(G, pos, ax=ax1, alpha=0.4, width=e_widths, edge_color="gray")
        nx.draw_networkx_nodes(G, pos, ax=ax1,
                               node_size=[d * 150 for d in node_deg],
                               node_color="skyblue", alpha=0.85,
                               edgecolors="darkblue", linewidths=1)
        nx.draw_networkx_labels(G, pos, ax=ax1, font_size=9, font_weight="bold", font_family=FONT_NAME)
        ax1.set_title("ë™ì‹œì¶œí˜„ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬", fontsize=16, fontweight="bold")
        ax1.axis("off")
        plt.tight_layout()
        st.pyplot(fig1)
        st.download_button("ğŸ–¼ ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ PNG",
                           save_fig_png(fig1), "network_basic.png", "image/png")
        plt.close(fig1)

    # 2. ì—°ê²° ì¤‘ì‹¬ì„±
    with net_tab2:
        cent_vals  = [degree_cent[n] for n in G.nodes()]
        node_size2 = [degree_cent[n] * 3000 + 200 for n in G.nodes()]

        fig2, ax2 = plt.subplots(figsize=(14, 10))
        nx.draw_networkx_edges(G, pos, ax=ax2, alpha=0.3, width=e_widths, edge_color="lightgray")
        nodes2 = nx.draw_networkx_nodes(G, pos, ax=ax2,
                                        node_size=node_size2, node_color=cent_vals,
                                        cmap=plt.cm.YlOrRd, alpha=0.9,
                                        edgecolors="black", linewidths=1)
        nx.draw_networkx_labels(G, pos, ax=ax2, font_size=9, font_weight="bold", font_family=FONT_NAME)
        plt.colorbar(nodes2, ax=ax2, label="ì—°ê²° ì¤‘ì‹¬ì„±", shrink=0.8)
        ax2.set_title("ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ â€” ì—°ê²° ì¤‘ì‹¬ì„±", fontsize=16, fontweight="bold")
        ax2.axis("off")
        plt.tight_layout()
        st.pyplot(fig2)
        st.download_button("ğŸ–¼ ì—°ê²° ì¤‘ì‹¬ì„± PNG",
                           save_fig_png(fig2), "network_centrality.png", "image/png")
        plt.close(fig2)

        # ì¤‘ì‹¬ì„± í…Œì´ë¸”
        st.markdown("##### ì—°ê²° ì¤‘ì‹¬ì„± ìƒìœ„ 20ê°œ ë‹¨ì–´")
        cent_df = (
            pd.DataFrame({"ë‹¨ì–´": list(degree_cent.keys()),
                          "ì—°ê²° ì¤‘ì‹¬ì„±": list(degree_cent.values()),
                          "ì—°ê²° ìˆ˜": [G.degree(n) for n in degree_cent]})
            .sort_values("ì—°ê²° ì¤‘ì‹¬ì„±", ascending=False)
            .head(20)
            .reset_index(drop=True)
        )
        cent_df.insert(0, "ìˆœìœ„", range(1, len(cent_df) + 1))
        cent_df["ì—°ê²° ì¤‘ì‹¬ì„±"] = cent_df["ì—°ê²° ì¤‘ì‹¬ì„±"].round(4)
        st.dataframe(cent_df, use_container_width=True, hide_index=True)

    # 3. ì»¤ë®¤ë‹ˆí‹°
    with net_tab3:
        fig3, ax3 = plt.subplots(figsize=(14, 10))
        nx.draw_networkx_edges(G, pos, ax=ax3, alpha=0.3, width=e_widths, edge_color="lightgray")
        nx.draw_networkx_nodes(G, pos, ax=ax3,
                               node_size=[G.degree(n) * 150 for n in G.nodes()],
                               node_color=comm_colors, cmap=plt.cm.Set1,
                               alpha=0.9, edgecolors="black", linewidths=1)
        nx.draw_networkx_labels(G, pos, ax=ax3, font_size=9, font_weight="bold", font_family=FONT_NAME)
        ax3.set_title(f"ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ â€” ì»¤ë®¤ë‹ˆí‹° íƒì§€ ({len(communities)}ê°œ ê·¸ë£¹)",
                      fontsize=16, fontweight="bold")
        ax3.axis("off")
        plt.tight_layout()
        st.pyplot(fig3)
        st.download_button("ğŸ–¼ ì»¤ë®¤ë‹ˆí‹° ë„¤íŠ¸ì›Œí¬ PNG",
                           save_fig_png(fig3), "network_community.png", "image/png")
        plt.close(fig3)

        # ì»¤ë®¤ë‹ˆí‹°ë³„ ì£¼ìš” ë‹¨ì–´
        st.markdown("##### ì»¤ë®¤ë‹ˆí‹°ë³„ ì£¼ìš” ë‹¨ì–´ (ì—°ê²° ìˆ˜ ê¸°ì¤€ ìƒìœ„ 5ê°œ)")
        comm_rows = []
        for ci, comm in enumerate(communities):
            top5 = sorted(comm, key=lambda x: G.degree(x), reverse=True)[:5]
            comm_rows.append({"ê·¸ë£¹": f"ê·¸ë£¹ {ci+1}", "ë‹¨ì–´ ìˆ˜": len(comm),
                              "ì£¼ìš” ë‹¨ì–´": ", ".join(top5)})
        st.dataframe(pd.DataFrame(comm_rows), use_container_width=True, hide_index=True)

    # 4. í•µì‹¬ ë„¤íŠ¸ì›Œí¬
    with net_tab4:
        G_core = nx.Graph()
        for (w1, w2), wt in cooc_counter.most_common(cooc_core_edges):
            G_core.add_edge(w1, w2, weight=wt)

        pos_core  = nx.spring_layout(G_core, k=3, iterations=100, seed=42)
        ew_core   = [G_core[u][v]["weight"] * 0.05 for u, v in G_core.edges()]
        ns_core   = [G_core.degree(n) * 300 for n in G_core.nodes()]
        el_core   = {(u, v): G_core[u][v]["weight"] for u, v in G_core.edges()}

        fig4, ax4 = plt.subplots(figsize=(13, 9))
        nx.draw_networkx_edges(G_core, pos_core, ax=ax4, alpha=0.55,
                               width=ew_core, edge_color="steelblue")
        nx.draw_networkx_nodes(G_core, pos_core, ax=ax4,
                               node_size=ns_core, node_color="lightcoral",
                               alpha=0.9, edgecolors="darkred", linewidths=2)
        nx.draw_networkx_labels(G_core, pos_core, ax=ax4, font_size=10, font_weight="bold", font_family=FONT_NAME)
        nx.draw_networkx_edge_labels(G_core, pos_core, edge_labels=el_core,
                                     ax=ax4, font_size=7)
        ax4.set_title(f"í•µì‹¬ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ (ìƒìœ„ {cooc_core_edges}ê°œ ì—°ê²°)",
                      fontsize=16, fontweight="bold")
        ax4.axis("off")
        plt.tight_layout()
        st.pyplot(fig4)
        st.download_button("ğŸ–¼ í•µì‹¬ ë„¤íŠ¸ì›Œí¬ PNG",
                           save_fig_png(fig4), "network_core.png", "image/png")
        plt.close(fig4)

    # 5. ì›í˜• ë ˆì´ì•„ì›ƒ
    with net_tab5:
        pos_circ = nx.circular_layout(G)

        fig5, ax5 = plt.subplots(figsize=(14, 14))
        nx.draw_networkx_edges(G, pos_circ, ax=ax5, alpha=0.2, width=0.5, edge_color="gray")
        nx.draw_networkx_nodes(G, pos_circ, ax=ax5,
                               node_size=[G.degree(n) * 100 for n in G.nodes()],
                               node_color=comm_colors, cmap=plt.cm.Set2,
                               alpha=0.9, edgecolors="black", linewidths=1)
        nx.draw_networkx_labels(G, pos_circ, ax=ax5, font_size=8, font_family=FONT_NAME)
        ax5.set_title("ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ â€” ì›í˜• ë ˆì´ì•„ì›ƒ", fontsize=16, fontweight="bold")
        ax5.axis("off")
        plt.tight_layout()
        st.pyplot(fig5)
        st.download_button("ğŸ–¼ ì›í˜• ë ˆì´ì•„ì›ƒ PNG",
                           save_fig_png(fig5), "network_circular.png", "image/png")
        plt.close(fig5)

    st.divider()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ì—°ë„ë³„ ë™ì‹œì¶œí˜„ ë¶„ì„ (5_cooccurrence_yearly)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    st.markdown("#### ğŸ“… ì—°ë„ë³„ ë™ì‹œì¶œí˜„ ë¶„ì„")

    cooc_yr_targets = sorted(
        df_work[year_col].dropna().astype(str).str.strip().unique().tolist()
    )

    with st.spinner(f"ì—°ë„ë³„ ë™ì‹œì¶œí˜„ ê³„ì‚° ì¤‘â€¦ ({len(cooc_yr_targets)}ê°œ ì—°ë„)"):
        yearly_cooc   = {}        # {year: Counter}
        yr_edge_rows  = []        # ì—°ë„ë³„ ì—£ì§€ CSVìš©
        yr_top10_rows = []        # ì—°ë„ë³„ ìƒìœ„10 CSVìš©

        for yr in cooc_yr_targets:
            yr_mask  = df_work[year_col].astype(str).str.strip() == yr
            yr_texts = df_work[yr_mask][text_col].dropna().astype(str).tolist()

            yr_noun_sets = []
            for txt in yr_texts:
                try:
                    res = kiwi.analyze(str(txt))
                    ns  = {tok for tok, tag, _, _ in res[0][0]
                           if tag in NOUN_TAGS and len(tok) >= min_len and tok not in stopwords_set}
                    yr_noun_sets.append(ns)
                except Exception:
                    yr_noun_sets.append(set())

            yr_cooc = Counter()
            for ns in yr_noun_sets:
                filtered = sorted(ns & top_word_set)
                for pair in combinations(filtered, 2):
                    yr_cooc[pair] += 1

            yearly_cooc[yr] = yr_cooc

            for (w1, w2), cnt in yr_cooc.most_common(100):
                yr_edge_rows.append({"ì—°ë„": yr, "source": w1, "target": w2, "weight": cnt})
            for rank, ((w1, w2), cnt) in enumerate(yr_cooc.most_common(10), 1):
                yr_top10_rows.append({"ì—°ë„": yr, "ìˆœìœ„": rank,
                                      "ë‹¨ì–´1": w1, "ë‹¨ì–´2": w2, "ë¹ˆë„": cnt})

    # â”€â”€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    yr_edge_df  = pd.DataFrame(yr_edge_rows)
    yr_top10_df = pd.DataFrame(yr_top10_rows)

    dl_ye1, dl_ye2 = st.columns(2)
    with dl_ye1:
        st.download_button(
            "â¬‡ ì—°ë„ë³„ ì—£ì§€ ë¦¬ìŠ¤íŠ¸ CSV",
            yr_edge_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            "cooccurrence_yearly_edges.csv", "text/csv", use_container_width=True,
        )
    with dl_ye2:
        st.download_button(
            "â¬‡ ì—°ë„ë³„ ìƒìœ„ 10 ìŒ CSV",
            yr_top10_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
            "cooccurrence_yearly_top10.csv", "text/csv", use_container_width=True,
        )

    st.divider()

    # â”€â”€ ì—°ë„ë³„ ê°œë³„ ë„¤íŠ¸ì›Œí¬ (2ì—´ ê·¸ë¦¬ë“œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("##### ì—°ë„ë³„ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„")
    n_cols2 = 2
    yr_chunks2 = [cooc_yr_targets[i:i+n_cols2] for i in range(0, len(cooc_yr_targets), n_cols2)]

    for chunk in yr_chunks2:
        cols2 = st.columns(n_cols2)
        for ci, yr in enumerate(chunk):
            yr_cooc = yearly_cooc.get(yr, Counter())
            if len(yr_cooc) < 5:
                with cols2[ci]:
                    st.caption(f"{yr}ë…„: ë°ì´í„° ë¶€ì¡±")
                continue

            Gy = nx.Graph()
            for (w1, w2), wt in yr_cooc.most_common(50):
                Gy.add_edge(w1, w2, weight=wt)
            if len(Gy.nodes()) < 3:
                continue

            pos_y    = nx.spring_layout(Gy, k=2, iterations=50, seed=42)
            ns_y     = [Gy.degree(n) * 150 for n in Gy.nodes()]
            ew_y     = [Gy[u][v]["weight"] * 0.1 for u, v in Gy.edges()]
            dc_y     = nx.degree_centrality(Gy)
            nc_y     = [dc_y[n] for n in Gy.nodes()]

            fig_y, ax_y = plt.subplots(figsize=(6, 5))
            nx.draw_networkx_edges(Gy, pos_y, ax=ax_y, alpha=0.4, width=ew_y, edge_color="gray")
            nd_y = nx.draw_networkx_nodes(Gy, pos_y, ax=ax_y, node_size=ns_y,
                                          node_color=nc_y, cmap=plt.cm.YlOrRd,
                                          alpha=0.9, edgecolors="black", linewidths=1)
            nx.draw_networkx_labels(Gy, pos_y, ax=ax_y, font_size=8, font_weight="bold", font_family=FONT_NAME)
            plt.colorbar(nd_y, ax=ax_y, label="ì—°ê²° ì¤‘ì‹¬ì„±", shrink=0.7)
            ax_y.set_title(f"{yr}ë…„ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬", fontsize=11, fontweight="bold")
            ax_y.axis("off")
            plt.tight_layout()
            with cols2[ci]:
                st.pyplot(fig_y)
                st.download_button(f"ğŸ–¼ {yr}ë…„ PNG", save_fig_png(fig_y),
                                   f"network_{yr}.png", "image/png",
                                   use_container_width=True)
            plt.close(fig_y)

    st.divider()

    # â”€â”€ ì—°ë„ë³„ ë¹„êµ ê·¸ë¦¬ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("##### ì—°ë„ë³„ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ")
    n_grid_cols = min(4, len(cooc_yr_targets))
    n_grid_rows = -(-len(cooc_yr_targets) // n_grid_cols)   # ceiling division

    fig_cmp, axes_cmp = plt.subplots(n_grid_rows, n_grid_cols,
                                     figsize=(n_grid_cols * 6, n_grid_rows * 5))
    axes_flat = axes_cmp.flatten() if hasattr(axes_cmp, "flatten") else [axes_cmp]

    for idx, yr in enumerate(cooc_yr_targets):
        ax_c = axes_flat[idx]
        yr_cooc = yearly_cooc.get(yr, Counter())
        if len(yr_cooc) < 5:
            ax_c.text(0.5, 0.5, f"{yr}ë…„\në°ì´í„° ë¶€ì¡±",
                      ha="center", va="center", fontsize=12)
            ax_c.axis("off")
            continue

        Gc = nx.Graph()
        for (w1, w2), wt in yr_cooc.most_common(30):
            Gc.add_edge(w1, w2, weight=wt)
        if len(Gc.nodes()) < 3:
            ax_c.axis("off")
            continue

        pos_c = nx.spring_layout(Gc, k=2, iterations=30, seed=42)
        nx.draw_networkx_edges(Gc, pos_c, ax=ax_c, alpha=0.3, width=0.5, edge_color="gray")
        nx.draw_networkx_nodes(Gc, pos_c, ax=ax_c,
                               node_size=[Gc.degree(n) * 80 for n in Gc.nodes()],
                               node_color="coral", alpha=0.85,
                               edgecolors="darkred", linewidths=0.5)
        nx.draw_networkx_labels(Gc, pos_c, ax=ax_c, font_size=7, font_weight="bold", font_family=FONT_NAME)
        ax_c.set_title(f"{yr}ë…„", fontsize=13, fontweight="bold")
        ax_c.axis("off")

    for idx in range(len(cooc_yr_targets), len(axes_flat)):
        axes_flat[idx].axis("off")

    yr_range2 = f"{min(cooc_yr_targets)}~{max(cooc_yr_targets)}"
    fig_cmp.suptitle(f"ì—°ë„ë³„ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ë¹„êµ ({yr_range2})",
                     fontsize=16, fontweight="bold", y=1.01)
    plt.tight_layout()
    st.pyplot(fig_cmp)
    st.download_button("ğŸ–¼ ì—°ë„ë³„ ë¹„êµ ê·¸ë¦¬ë“œ PNG", save_fig_png(fig_cmp),
                       "network_yearly_comparison.png", "image/png")
    plt.close(fig_cmp)

    st.divider()

    # â”€â”€ ì£¼ìš” ë‹¨ì–´ ìŒ ì—°ë„ë³„ íŠ¸ë Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("##### ì£¼ìš” ë‹¨ì–´ ìŒ ì—°ë„ë³„ íŠ¸ë Œë“œ")

    total_cooc_all = Counter()
    for cnt in yearly_cooc.values():
        total_cooc_all.update(cnt)
    top_pairs_trend = [pair for pair, _ in total_cooc_all.most_common(10)]

    trend_rows = []
    for pair in top_pairs_trend:
        for yr in cooc_yr_targets:
            trend_rows.append({
                "ë‹¨ì–´ìŒ": f"{pair[0]}-{pair[1]}",
                "ì—°ë„": yr,
                "ë¹ˆë„": yearly_cooc[yr].get(pair, 0),
            })
    trend_df = pd.DataFrame(trend_rows)

    fig_tr, ax_tr = plt.subplots(figsize=(12, 6))
    for pair in top_pairs_trend[:6]:
        pname = f"{pair[0]}-{pair[1]}"
        d = trend_df[trend_df["ë‹¨ì–´ìŒ"] == pname]
        ax_tr.plot(d["ì—°ë„"], d["ë¹ˆë„"], marker="o", linewidth=2, markersize=6, label=pname)

    ax_tr.set_xlabel("ì—°ë„", fontsize=11)
    ax_tr.set_ylabel("ë™ì‹œì¶œí˜„ ë¹ˆë„", fontsize=11)
    ax_tr.set_title("ì£¼ìš” ë‹¨ì–´ ìŒ ì—°ë„ë³„ ë™ì‹œì¶œí˜„ íŠ¸ë Œë“œ", fontsize=14, fontweight="bold")
    ax_tr.legend(bbox_to_anchor=(1.02, 1), loc="upper left", fontsize=9)
    ax_tr.grid(True, alpha=0.3)
    plt.tight_layout()
    st.pyplot(fig_tr)

    # íŠ¸ë Œë“œ CSV ë‹¤ìš´ë¡œë“œ
    trend_pivot = trend_df.pivot(index="ì—°ë„", columns="ë‹¨ì–´ìŒ", values="ë¹ˆë„").fillna(0).astype(int)
    dl_trend1, dl_trend2 = st.columns([1, 3])
    with dl_trend1:
        st.download_button(
            "â¬‡ íŠ¸ë Œë“œ CSV",
            trend_pivot.to_csv(encoding="utf-8-sig").encode("utf-8-sig"),
            "cooccurrence_trend.csv", "text/csv", use_container_width=True,
        )
    plt.close(fig_tr)

